{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(path=\"config/pipeline_config.yaml\"):\n",
    "    with open(path, \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def write_parquet(df: DataFrame, output_path: str, partition_by: str = None, mode: str = \"overwrite\"):\n",
    "    try:\n",
    "        if partition_by:\n",
    "            df.write.mode(mode).partitionBy(partition_by).parquet(output_path)\n",
    "        else:\n",
    "            df.write.mode(mode).parquet(output_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"write_parquet: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "\n",
    "def create_spark_session(app_name):\n",
    "    config = load_config()\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(config[\"spark\"][\"master\"]) \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def get_schema():\n",
    "    return StructType([\n",
    "        StructField(\"Unnamed: 0\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"long_description\", StringType(), True),\n",
    "        StructField(\"customer_part_id\", StringType(), True),\n",
    "        StructField(\"manufacturer_name\", StringType(), True),\n",
    "        StructField(\"manufacturer_part_id\", StringType(), True),\n",
    "        StructField(\"competitor_name\", StringType(), True),\n",
    "        StructField(\"competitor_part_name\", StringType(), True),\n",
    "        StructField(\"competitor_part_id\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"unit_of_measure\", StringType(), True),\n",
    "        StructField(\"unit_quantity\", StringType(), True),\n",
    "        StructField(\"requested_quantity\", StringType(), True),\n",
    "        StructField(\"requested_unit_price\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def read_materials(spark, input_path, header, quote, escape, multiline, infer_schema, file_format):\n",
    "    schema = get_schema() if not infer_schema else None\n",
    "    try:\n",
    "        df = spark.read.format(file_format) \\\n",
    "            .option(\"header\", header) \\\n",
    "            .option(\"quote\", quote) \\\n",
    "            .option(\"escape\", escape) \\\n",
    "            .option(\"multiline\", multiline)\n",
    "        \n",
    "        if infer_schema:\n",
    "            df = df.option(\"inferSchema\", True)\n",
    "        else:\n",
    "            df = df.schema(schema)\n",
    "        \n",
    "        df = df.load(input_path)\n",
    "        print(\"File loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Ingestion process failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, regexp_replace, when, initcap, count, lit\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "def clean_and_validate_data(df: DataFrame, quarantine_path: str) -> DataFrame:\n",
    "    try:\n",
    "        # Identify rows with NaN values\n",
    "        number_columns = [\"id\", \"unit_quantity\", \"requested_quantity\", \"requested_unit_price\"]\n",
    "        nan_condition = [~col(c).rlike(r\"^\\d+\\.0$\") for c in number_columns]\n",
    "        only_nan_rows = df.filter(reduce(lambda a, b: a & b, nan_condition))\n",
    "\n",
    "        # Identify all columns null\n",
    "        null_condition = [col(c).isNull() for c in df.columns]\n",
    "        only_null_rows = df.filter(reduce(lambda a, b: a & b, null_condition))\n",
    "\n",
    "        # Clean spaces, normalize text\n",
    "        text_columns = [\"name\", \"description\", \"manufacturer_name\", \"competitor_name\", \"category\"]\n",
    "        for col_name in text_columns:\n",
    "            df = df.withColumn(col_name, initcap(col(col_name)))\n",
    "            df = df.withColumn(col_name, trim(regexp_replace(col(col_name), r\"\\s+\", \" \")))\n",
    "\n",
    "        # Validate columns that can't be null\n",
    "        invalid_rows = df.filter(\n",
    "            col(\"id\").isNull() |\n",
    "            col(\"name\").isNull() |\n",
    "            col(\"category\").isNull()\n",
    "        )\n",
    "\n",
    "        discarded_rows = only_nan_rows.union(only_null_rows)\n",
    "        discarded_rows = discarded_rows.union(invalid_rows).distinct()\n",
    "        discarded_rows = discarded_rows.repartition(1) \n",
    "\n",
    "\n",
    "        #  Save discarded rows in quarantine\n",
    "        if discarded_rows.count() > 0:\n",
    "            discarded_rows.write.mode(\"overwrite\").parquet(quarantine_path)\n",
    "            print(f\"{discarded_rows.count()} rows sent to quarantine.\")\n",
    "        \n",
    "        # Filter valid rows\n",
    "        valid_rows = df.subtract(discarded_rows)\n",
    "\n",
    "        # Cast to integer\n",
    "        valid_rows = valid_rows.withColumn(\"id\", col(\"id\").cast(IntegerType()))\n",
    "\n",
    "        # Cast to double\n",
    "        double_columns = [\"unit_quantity\", \"requested_quantity\", \"requested_unit_price\"]\n",
    "        for c in double_columns:\n",
    "            valid_rows = valid_rows.withColumn(c, col(c).cast(DoubleType()))\n",
    "       \n",
    "        # Drop columns > 90% null\n",
    "        total_rows = valid_rows.count()\n",
    "        threshold = 0.9\n",
    "\n",
    "        null_counts = valid_rows.select([\n",
    "            (count(when(col(c).isNull(), c)) / lit(total_rows)).alias(c)\n",
    "            for c in valid_rows.columns\n",
    "        ])\n",
    "        null_ratios = null_counts.first().asDict()\n",
    "        columns_to_keep = [c for c, ratio in null_ratios.items() if ratio < threshold]\n",
    "        df_clean = valid_rows.select(columns_to_keep)\n",
    "\n",
    "        df_clean = df_clean.drop(\"Unnamed: 0\")\n",
    "\n",
    "        return df_clean\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"clean_and_validate_data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, avg, min, max, countDistinct, when, lit, isnan, round\n",
    "\n",
    "def category_aggregations(df: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        return df.groupBy(\"category\").agg(\n",
    "            count(\"*\").alias(\"item_count\"),\n",
    "            round(avg(\"requested_unit_price\"), 2).alias(\"avg_price\"),\n",
    "            round(min(\"requested_unit_price\"), 2).alias(\"min_price\"),\n",
    "            round(max(\"requested_unit_price\"), 2).alias(\"max_price\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"unexpected error has occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def column_profiling(df: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        # Metrics\n",
    "        stats = []\n",
    "        for col_name in df.columns:\n",
    "            col_stats = df.select(\n",
    "                lit(col_name).alias(\"column\"),\n",
    "                count(when(col(col_name).isNull() | isnan(col(col_name)), col_name)).alias(\"null_count\"),\n",
    "                countDistinct(col_name).alias(\"unique_count\"),\n",
    "                min(col_name).alias(\"min\"),\n",
    "                max(col_name).alias(\"max\")\n",
    "            ).collect()[0]\n",
    "            stats.append(col_stats.asDict())\n",
    "        \n",
    "        # Convertimos a un nuevo DataFrame\n",
    "        return df.sparkSession.createDataFrame(stats)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error has occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "spark = create_spark_session(config[\"spark\"][\"app_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = config[\"paths\"]\n",
    "input_path = paths[\"input_csv\"]\n",
    "raw_path = paths[\"raw_data_dir\"]\n",
    "quarantine_path = paths[\"quarantine_data_dir\"]\n",
    "processed_path = paths[\"processed_data_dir\"]\n",
    "\n",
    "read_opts = config.get(\"read_options\", {})\n",
    "\n",
    "header = read_opts.get(\"header\", True)\n",
    "infer_schema = read_opts.get(\"infer_schema\", False)\n",
    "delimiter = read_opts.get(\"delimiter\", \",\")\n",
    "multiline = read_opts.get(\"multiline\", False)\n",
    "escape = read_opts.get(\"escape\", '\"')\n",
    "quote = read_opts.get(\"quote\", '\"')\n",
    "file_format = read_opts.get(\"file_format\", \"csv\")\n",
    "\n",
    "# header = config[\"read_options\"][\"header\"]\n",
    "# delimiter = config[\"read_options\"][\"delimiter\"]\n",
    "# delimiter = config[\"read_options\"][\"delimiter\"]\n",
    "# multiline = config[\"read_options\"][\"multiline\"]\n",
    "# escape = config[\"read_options\"][\"delimiter\"]\n",
    "# quote = config[\"read_options\"][\"quote\"]\n",
    "# file_format = config[\"read_options\"][\"file_format\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ingestion\n",
    "df_raw = read_materials(spark, input_path, header, quote, escape, multiline, infer_schema, file_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 rows sent to quarantine.\n"
     ]
    }
   ],
   "source": [
    "# Cleansing and validation\n",
    "df_clean = clean_and_validate_data(df_raw, quarantine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "|Unnamed: 0|  id|                name|         description|long_description|customer_part_id|manufacturer_name|manufacturer_part_id|competitor_name|competitor_part_name|competitor_part_id|category|unit_of_measure|unit_quantity|requested_quantity|requested_unit_price|\n",
      "+----------+----+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "|       0.0| 1.0|3COM ETHERLINK 3 ...|3COM ETHERLINK 3 ...|            NULL|            NULL|            3 COM|               3C509|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       1.0| 2.0|Network Card, Eth...|Network Card, Eth...|            NULL|            NULL|            3 COM|              3C509B|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       2.0| 3.0|CARD,ETHERLINK II...|CARD,ETHERLINK II...|            NULL|            NULL|            3 COM|        3c509b-combo|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       3.0| 4.0|  ETHERLINK III CARD|  ETHERLINK III CARD|            NULL|            NULL|            3 COM|           3C509B-TP|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       4.0| 5.0|3COM FAST ETHERLI...|3COM FAST ETHERLI...|            NULL|            NULL|            3 COM|            3C515-TX|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       5.0| 6.0|3COM Etherlink II...|3COM Etherlink II...|            NULL|            NULL|            3 COM|               3C562|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       6.0| 7.0|Card,Etherlink,PC...|Card,Etherlink,PC...|            NULL|            NULL|            3 COM|         3C905C-TX-M|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       7.0| 8.0|CRD,PC,3COM,10MBP...|CRD,PC,3COM,10MBP...|            NULL|            NULL|            3 COM|           3CCE589ET|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       8.0| 9.0|         TOUCHSCREEN|         TOUCHSCREEN|            NULL|            NULL|               3M|     11-71315-225-00|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|              1150.0|\n",
      "|       9.0|10.0|LCD TOUCH PANEL 1...|LCD TOUCH PANEL 1...|            NULL|            NULL|               3M|     11-71315-225-01|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|              1150.0|\n",
      "+----------+----+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.coalesce(1).write.mode(\"overwrite\").json(\"data/materials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_strings = df_raw.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+---+-----+\n",
      "|              column|null_count|unique_count|min|  avg|\n",
      "+--------------------+----------+------------+---+-----+\n",
      "|requested_unit_price|         1|        2247|0.0|464.0|\n",
      "+--------------------+----------+------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.select(lit(\"requested_unit_price\").alias(\"column\"),\n",
    "                count(when(col(\"requested_unit_price\").isNull() | isnan(col(\"requested_unit_price\")), \"requested_unit_price\")).alias(\"null_count\"),\n",
    "                countDistinct(\"requested_unit_price\").alias(\"unique_count\"),\n",
    "                min(\"requested_unit_price\").alias(\"min\"),\n",
    "                round(avg(\"requested_unit_price\"), 2).alias(\"avg\"))\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "df_category_stats = category_aggregations(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, min, max, mean, when, isnan\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Paso 1: Preparamos agregaciones por columna\n",
    "agg_exprs = []\n",
    "for c in df_clean.columns:\n",
    "    agg_exprs += [\n",
    "        count(when(col(c).isNull() | isnan(col(c)), c)).alias(f\"{c}_nulls\"),\n",
    "        countDistinct(col(c)).alias(f\"{c}_unique\"),\n",
    "        min(col(c)).alias(f\"{c}_min\"),\n",
    "        max(col(c)).alias(f\"{c}_max\"),\n",
    "        mean(col(c)).alias(f\"{c}_mean\")\n",
    "    ]\n",
    "\n",
    "# Paso 2: Ejecutamos una sola agregaci√≥n\n",
    "agg_result = df_clean.agg(*agg_exprs).first().asDict()\n",
    "\n",
    "# Paso 3: Armamos los resultados por columna\n",
    "stats = []\n",
    "for c in df_clean.columns:\n",
    "    stats.append(Row(\n",
    "        column=c,\n",
    "        dtype=str(df_clean.schema[c].dataType),\n",
    "        null_count=agg_result.get(f\"{c}_nulls\"),\n",
    "        unique_count=agg_result.get(f\"{c}_unique\"),\n",
    "        min=agg_result.get(f\"{c}_min\"),\n",
    "        max=agg_result.get(f\"{c}_max\"),\n",
    "        mean=agg_result.get(f\"{c}_mean\")\n",
    "    ))\n",
    "\n",
    "# Paso 4: Convertimos a DataFrame final\n",
    "summary_df = spark.createDataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet(df_category_stats, os.path.join(processed_path, \"category_stats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.write.mode(\"overwrite\").partitionBy(\"category\").orc(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = spark.read \\\n",
    "    .parquet(\"data/raw\")\n",
    "\n",
    "df_q = spark.read \\\n",
    "    .parquet(\"data/quarantine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+--------+\n",
      "|Unnamed: 0|    id|name|description|long_description|customer_part_id|manufacturer_name|manufacturer_part_id|competitor_name|competitor_part_name|competitor_part_id|category|unit_of_measure|unit_quantity|requested_quantity|requested_unit_price|is_valid|\n",
      "+----------+------+----+-----------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+--------+\n",
      "|    1052.0|1053.0|NULL|       NULL|            NULL|            NULL|             AJAX|           U-01-0231|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|               880.0|       N|\n",
      "|    1224.0|1225.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|         2099-BM08-S|           NULL|                NULL|              NULL|  DRIVES|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2050.0|2051.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|          1340-50510|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2052.0|2053.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|         1352-506992|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2054.0|2055.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|             1352-54|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2505.0|2506.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|             1725-79|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2547.0|2548.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|          1734-OBE2E|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    2911.0|2912.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1756-L71S|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|              2530.0|       N|\n",
      "|    2916.0|2917.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1756-L73S|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|              6490.0|       N|\n",
      "|    2918.0|2919.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1756-L74S|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|              6710.0|       N|\n",
      "|    2923.0|2924.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1756-L83E|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|              7249.0|       N|\n",
      "|    3027.0|3028.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|          1764-288XB|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3329.0|3330.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|         1771-IX-CAL|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3447.0|3448.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1771-P4-R|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3749.0|3750.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|         1784-PCMC/B|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3750.0|3751.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|           1784-PCMK|           NULL|                NULL|              NULL|     ISG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3829.0|3830.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|        1785-L410B/E|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    3972.0|3973.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|             2000-42|           NULL|                NULL|              NULL|     MRT|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    4570.0|4571.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|   20DD052A0EYNARANE|           NULL|                NULL|              NULL|  DRIVES|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "|    4768.0|4769.0|NULL|       NULL|            NULL|            NULL|    ALLEN BRADLEY|    22AD8P0A0AYNNNC0|           NULL|                NULL|              NULL|  DRIVES|           NULL|         NULL|              NULL|                 0.0|       N|\n",
      "+----------+------+----+-----------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#45432\n",
    "#726\n",
    "df_q.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "|Unnamed: 0| id|                name|         description|long_description|customer_part_id|manufacturer_name|manufacturer_part_id|competitor_name|competitor_part_name|competitor_part_id|category|unit_of_measure|unit_quantity|requested_quantity|requested_unit_price|\n",
      "+----------+---+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "|       0.0|1.0|3COM ETHERLINK 3 ...|3COM ETHERLINK 3 ...|            NULL|            NULL|            3 COM|               3C509|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       1.0|2.0|Network Card, Eth...|Network Card, Eth...|            NULL|            NULL|            3 COM|              3C509B|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       2.0|3.0|CARD,ETHERLINK II...|CARD,ETHERLINK II...|            NULL|            NULL|            3 COM|        3c509b-combo|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       3.0|4.0|  ETHERLINK III CARD|  ETHERLINK III CARD|            NULL|            NULL|            3 COM|           3C509B-TP|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "|       4.0|5.0|3COM FAST ETHERLI...|3COM FAST ETHERLI...|            NULL|            NULL|            3 COM|            3C515-TX|           NULL|                NULL|              NULL|     OAG|           NULL|         NULL|              NULL|                 0.0|\n",
      "+----------+---+--------------------+--------------------+----------------+----------------+-----------------+--------------------+---------------+--------------------+------------------+--------+---------------+-------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "text_columns = [\"name\", \"description\", \"manufacturer_name\", \"competitor_name\", \"category\"]\n",
    "for col_name in text_columns:\n",
    "    cleaned_col = col_name + \"_cleaned\"\n",
    "    df = df_raw.withColumn(cleaned_col, initcap(col(col_name)))\n",
    "    df = df_raw.withColumn(cleaned_col, trim(regexp_replace(col(col_name), r\"\\s+\", \" \")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
