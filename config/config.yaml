# Files path
paths:
  input_file: "data/materials-2.csv"
  schema_report_path: "data/logs"
  cleaning_log_path: "data/logs"
  raw_data_dir: "data/raw/"
  processed_data_dir: "data/processed/"
  quarantine_data_dir: "data/quarantine/"

# General configurations
spark:
  app_name: "MaterialsDataPipeline"
  master: "local[1]"  # Change to yarn or cluster

# Loading options
read_options:
  header: true
  infer_schema: false
  delimiter: ","
  multiline: true
  escape: '"'
  quote: '"'
  read_file_format: "csv"

# Writing options
write_options:
  write_file_format: "parquet"
  mode: "overwrite"

# Checkpoint
checkpointing:
  use_checkpoint: false
  checkpoint_path: "data/checkpoints/df_clean"

# Logs
logs:
  log_name: "pipeline_log"
  log_file: "data/logs/pipeline.log"